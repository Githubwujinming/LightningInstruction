# 检查点checkpoint

训练时可以手动/自动保存多个检查点（比如last.ckpt, best.ckpt），可以用于后续的测试、恢复训练等。每个checkpoint 包含以下参数：

 - Global step 已运行的training_step数

 - LightningModule’s state_dict 模型权重 

 - State of all optimizers 优化器状态

 - State of all learning rate schedulers 学习率状态

 - State of all callbacks (for stateful callbacks) 回调内的状态

 - State of datamodule (for stateful datamodules)

 - The hyperparameters (init arguments) with which the model was created 模型的超参数

 - The hyperparameters (init arguments) with which the datamodule was created 数据集的超参数

 - State of Loops

## 保存checkpoint

Trainer 默认会自动保存，如果有callback则按其设置的条件保存，比如选择self.log记录的最高精度（如val/acc，val/f1）保存，n个epochs或step后自动保存。

```python
# 指定目录每个epoch后保存
trainer = Trainer(default_root_dir="some/path/")
```

## 加载checkpoint

调用**load_from_checkpoint**方法。

```python
model = MyLightningModule.load_from_checkpoint("/path/to/checkpoint.ckpt")

# disable randomness, dropout, etc...
model.eval()

# predict with the model
y_hat = model(x)
```

## 恢复训练
fit()时设置ckpt_path指向checkpoint文件路径
```python
model = LitModel()
trainer = Trainer()

# automatically restores model, epoch, step, LR schedulers, etc...
trainer.fit(model, ckpt_path="some/path/to/my_checkpoint.ckpt")
```